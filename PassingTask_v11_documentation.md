# PassingTask_v11.ipynb 解説ドキュメント

## 1. 概要

### 目的
2つの能動的推論（Active Inference）エージェントが、10×20のグリッド上で**互いに反対方向へ移動し、衝突を回避しながらすれ違う**シミュレーションです。

### エージェントの役割
-   **Agent0**: 下端 (y=0) → 上端 (y=19) へ移動することを好む
-   **Agent1**: 上端 (y=19) → 下端 (y=0) へ移動することを好む

### 特徴
各エージェントは**相手の状態（位置・行動）も内部モデルに含む**設計（Theory of Mind的要素）。相手の動きを推論・予測しながら、自身の目的（移動と衝突回避）を達成する行動を選択します。

---

## 2. コード構造

| セル | 内容 |
| :--- | :--- |
| 2-7 | **インポート**: JAX, PyMDP (JAX backend), pyglet, NumPy, Matplotlib等のライブラリ読み込み |
| 5 | **JAX設定**: デバイス確認とコンフィグ設定（CPU/TPU/CUDA） |
| 7 | **表示用関数**: `matrixDisplay`, `barPlotDisplay` など、行列やグラフを描画する関数群 |
| 8 | **A,B,C,D行列の設定**: 生成モデルの定義（状態空間、選好、初期信念など） |
| 9 | **JAX用にバッチ化**: 定義した行列をJAX配列に変換し、バッチ次元を追加 |
| 10 | **update_agent関数**: エージェントの推論・計画・学習を行うメイン関数（`@jit` で高速化） |
| 11 | **UI要素の初期化**: 可視化フラグの初期設定 |
| 12 | **エージェント初期化**: 2体の `AIFAgent` インスタンスを作成し、初期化 |
| 13 | **メインシミュレーションループ**: `pyglet` ウィンドウイベントループ、タイムステップ進行、描画、データ記録 |
| 15-16 | **データ保存/読込**: `pickle` を用いた状態の保存と読み込み |
| 17 | **CSV保存**: 結果データをCSV形式で出力 |
| 18-19 | **データプロット**: `matplotlib` を用いたエピソードごとの結果（成功率など）の可視化 |

---

## 3. 生成モデルの構造（セル8）

### 3.1 グリッドと状態空間

```python
x_division = 10      # x方向（幅）
y_division = 20      # y方向（高さ）
col_division = 10    # 衝突距離の離散化ビン数
actiondivision = 3   # 行動（-1: 左/下, 0: 静止, +1: 右/上）※各軸ごと
```

### 3.2 観測モダリティ（9つ）

```python
num_obs = [10, 20, 3, 3, 10, 20, 3, 3, 10]
```

| インデックス | 内容 | 次元数 | 依存する隠れ状態 |
| :--- | :--- | :--- | :--- |
| 0 | 自分のx座標 | 10 | 自分のx座標 (0) |
| 1 | 自分のy座標 | 20 | 自分のy座標 (1) |
| 2 | 自分のx方向行動 | 3 | 自分のx方向行動 (2) |
| 3 | 自分のy方向行動 | 3 | 自分のy方向行動 (3) |
| 4 | 相手のx座標 | 10 | 相手のx座標 (4) |
| 5 | 相手のy座標 | 20 | 相手のy座標 (5) |
| 6 | 相手のx方向行動 | 3 | 相手のx方向行動 (6) |
| 7 | 相手のy方向行動 | 3 | 相手のy方向行動 (7) |
| 8 | **衝突距離** | 10 | **自分と相手の位置 (0, 1, 4, 5)** |

### 3.3 隠れ状態因子（8つ）

```python
num_states = [10, 20, 3, 3, 10, 20, 3, 3]
```

| インデックス | 内容 | 次元数 |
| :--- | :--- | :--- |
| 0 | 自分のx座標 | 10 |
| 1 | 自分のy座標 | 20 |
| 2 | 自分のx方向行動 | 3 |
| 3 | 自分のy方向行動 | 3 |
| 4 | 相手のx座標 | 10 |
| 5 | 相手のy座標 | 20 |
| 6 | 相手のx方向行動 | 3 |
| 7 | 相手のy方向行動 | 3 |

### 3.4 A行列（尤度モデル）

-   **A_dependencies**: `[[0], [1], [2], [3], [4], [5], [6], [7], [0, 1, 4, 5]]`
-   モダリティ8（衝突距離）以外は、対応する状態因子から直接マッピング。
-   **モダリティ8（衝突距離）**は、自分と相手のX, Y座標（因子0, 1, 4, 5）の組み合わせから距離を計算してマッピング。

### 3.5 B行列（遷移モデル）

-   **B_dependencies**: `[[0], [1], [2], [3], [4], [5], [6], [7]]`
-   各状態因子は独立して遷移すると仮定。
-   実際には、エージェントの位置（因子0, 1, 4, 5）は行動（コントロールファクター）によって変化するように設定される。

### 3.6 C行列（選好/ゴール）

`M_PREF = -0.2` (コストの重み)

#### Agent0の選好
-   `C[0][0]`: 中央付近のx座標を好む（放物線状）
-   `C[0][1]`: y座標が大きい方（上）を好む
-   `C[0][2]`, `C[0][3]`: 特定の移動方向へ選好を持つ（慣性的な要素）
-   `C[0][5]`: 相手がy座標の小さい方（下）にいる状態を好む（すれ違い促進）
-   `C[0][8]`: **衝突距離が大きいこと（衝突回避）を強く好む**

#### Agent1の選好
-   `C[1][1]`: y座標が小さい方（下）を好む
-   `C[1][5]`: 相手がy座標の大きい方（上）にいる状態を好む
-   他はAgent0と同様に対称的な設定。

### 3.7 政策空間 (Policies)

```python
policies = [
    # Agent0用
    jnp.stack([
        np.array([[i, j, i, j, k, l, k, l]])
        for i in range(num_controls[0]) # 自分のx
        for j in [1, 2]                 # 自分のy (静止 or 上) ※後退制限
        for k in range(num_controls[4]) # 相手のx
        for l in [0, 1]                 # 相手のy (下 or 静止) ※相手の後退も想定しない
    ]),
    # Agent1用 (逆方向)
    ...
]
```

-   各政策は **[自分のx, 自分のy, ..., 相手のx, 相手のy]** という8次元の行動セット。
-   **特徴**: 自分の行動だけでなく、**相手がどう動くか**も政策の一部として評価する。

### 3.8 Alpha（モダリティ精度/注意の重み）

```python
instAlpha = 0.5
Alpha[0] = jnp.array([(1.-instAlpha)*2, ..., instAlpha*2, ..., 1.])
```
-   自分の状態に関連するモダリティと、相手の状態に関連するモダリティへの重み付け（注意の向け方）を調整。
-   デフォルトでは概ね均等あるいは `instAlpha` で調整可能。

---

## 4. update_agent関数（セル10）

エージェントの1ステップの処理を定義。JAXのJITコンパイルにより高速化されています。

```python
def update_agent(agents, outcomes, actions, infer_args, batch_keys, ...):
```

### 処理フロー
1.  **状態推論 (`infer_states_vfe` / `infer_states`)**:
    -   観測(`outcomes`)から現在の隠れ状態の信念(`beliefs`)を更新。VFE（変分自由エネルギー）を最小化。
2.  **政策推論 (`infer_policies_detail`)**:
    -   各政策のEFE（期待自由エネルギー）を計算。
    -   EFEは「有用性（ゴール達成） + 情報獲得（探索）」で構成。
3.  **行動サンプリング (`sample_action`)**:
    -   政策の事後分布(`q_pi`)から次の行動をサンプリング。
4.  **履歴更新**:
    -   計算効率のため、保持する履歴の長さ(`num_history`)を制限。
5.  **経験的事前分布の更新 (`update_empirical_prior`)**:
    -   次のステップの予測のためにD行列などを更新。
6.  **学習 (`infer_parameters`)**:
    -   `Learning_Flag` が有効な場合、経験に基づいてモデルパラメータ（`pA`, `pB`）を更新。

---

## 5. メインループ（セル13）

`while not EndFlag:` でエピソードごとのループ、その中で `while T < 50:` でタイムステップのループを実行。

### 5.1 初期化（エピソード開始時）
-   `agent_x`, `agent_y`: スタート位置 (5,0) と (5,19) にセット。
-   `agents[i].D`: 信念をリセット。
-   `agents[i].pA`, `agents[i].pB`: 学習したパラメータは維持（リセットする場合もあるコードになっているので注意）。

### 5.2 タイムステップ処理
1.  **環境更新**: エージェントの選択した行動に基づき、グリッド上の実際の位置を更新。
2.  **観測生成**: 位置をone-hotベクトル化し、距離を計算して観測(`obs`)を作成。
3.  **エージェント更新**: 両エージェントの `update_agent` を実行。
4.  **可視化 (`pyglet`)**:
    -   `Matrix_visualization`: 信念やA/B行列のヒートマップ表示。
    -   `Game_visualization`: グリッド上のエージェント位置表示。
    -   `Text_visualization`: VFE, EFE, ステータス情報の表示。
5.  **データ記録**: VFE, EFE, 位置などを記録。
6.  **終了判定**: ゴール到達、時間切れ、衝突などをチェック。

### 5.3 終了条件
-   **done**: 両方のエージェントが反対側の端に到達。
-   **out of time**: 50ステップ経過。
-   **forced to end**: ユーザーによる強制終了 (Esc)。
-   **interrupted**: ユーザーによるエピソードリセット (X)。

---

## 6. キーボード・マウス操作

| キー | 機能 |
| :--- | :--- |
| **Esc** | シミュレーション全体の強制終了 |
| **X** | 現在のエピソードをリセットして次へ |
| **E** | 「End If Done」モード切替（ゴール到達時に終了するか） |
| **A** | 行列・グラフの表示/非表示 (Matrix Visualization) |
| **M** | 表示行列の切替 (B行列 ⇔ pB(学習パラメータ)) |
| **T** | テキスト情報の表示/非表示 |
| **G** | ゲーム画面（グリッド）の表示/非表示 |
| **L** | 学習 (Learning) のON/OFF切替 |
| **R** | 学習率 (Learning Rate) の切替 (0.01 ⇔ 1.0) |
| **C** | 連続記録 (MetricRecord) の開始 |
| **Mouse Left** | 一時停止 / 再開 |

---

## 7. 出力データ

### 7.1 SaveData_Path (時系列データ)
各ステップごとの詳細データ。
-   `Episode`: エピソード番号
-   `T`: タイムステップ
-   `AgentID`: 0 or 1
-   `X`, `Y`: 位置
-   `EFE`: 期待自由エネルギー
-   `VFE`: 変分自由エネルギー
-   `EstimationMatch`: 相手の行動予測が当たったか
-   `Action`: 選択した行動

### 7.2 Reached_Analysis (エピソードサマリ)
エピソード終了ごとの集計データ。
-   `Episode`: エピソード番号
-   `AlphaPhase`: パラメータセットID
-   `T`: 終了にかかったステップ数
-   `AgentID`: エージェントID
-   `Min_distance`: エピソード中の最小距離（衝突危険度）
-   `VFE_avg/max`: VFE統計
-   `EFE_avg/max`: EFE統計
-   `Pred`: 行動予測精度
-   `EndReason`: 終了理由

---

## 8. 重要な変数・パラメータ

| 変数名 | 説明 | デフォルト値 |
| :--- | :--- | :--- |
| `x_division` / `y_division` | グリッドサイズ | 10 / 20 |
| `col_division` | 衝突距離の解像度 | 10 |
| `actiondivision` | 行動の次元数 (-1, 0, 1) | 3 |
| `num_history` | 推論に使う過去の履歴長 | 16 |
| `M_PREF` | 選好の強さ（コスト係数） | -0.2 |
| `instAlpha` | 自他への注意のバランス | 0.5 |
| `learningRateB` | B行列の学習率 | 0.01 |
| `PauseFlag` | 一時停止フラグ | False |
| `Learning_Flag` | 学習有効化フラグ | True |

---

## 9. 衝突判定と距離計算

衝突は直接的な座標一致だけでなく、「距離」として観測されます。

```python
# ユークリッド距離の計算
agent_distance = np.sqrt((agent_x[0] - agent_x[1])**2 + (agent_y[0] - agent_y[1])**2)

# 距離の離散化 (one-hot化のインデックス)
# 距離 * 2 をインデックスとし、最大値(9)でクリップ
col_idx = max(0, min(col_division-1, int(agent_distance*2)))
col_division_onehot = nn.one_hot(col_idx, col_division)
```
エージェントはこの `col_division_onehot` を観測し、これが大きい値（距離が遠い状態）になることを好むようにC行列で設定されています。

---

## 10. 学習の仕組み

### 学習対象 (`pA`, `pB`)
-   **pA (A行列のディリクレ分布パラメータ)**: 観測モデルの不確実性を学習。
-   **pB (B行列のディリクレ分布パラメータ)**: 遷移モデル（行動の結果や相手の動き方）を学習。

### プロセス
1.  `update_agent` 内で `infer_parameters` が呼ばれる。
2.  直前の `beliefs` (状態) と `outcomes` (観測)、`actions` (行動) の関係を用いて、パラメータを更新。
3.  これにより、「相手はこう動く傾向がある」「この距離だと衝突判定になる」といった知識が蓄積される。

---

## 11. 実行結果の目安

コンソール出力の例：
```
episode : 0
    T:21   minDis:4.24   end for done
episode : 1
    T:20   minDis:3.16   end for done
...
```
-   **T**: ゴールまでのステップ数。最短経路なら20ステップ程度。すれ違いで手間取ると増える。
-   **minDis**: 最も近づいた時の距離。これが小さすぎると衝突リスクが高い。
-   **end for done**: 無事ゴールに到達して終了。

---
